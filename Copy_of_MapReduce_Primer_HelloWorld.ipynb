{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://github.com/groda/big_data\"><div><img src=\"https://github.com/groda/big_data/blob/master/logo_bdb.png?raw=true\" align=right width=\"90\"></div></a>\n",
        "\n",
        "# MapReduce: A Primer with <code>Hello World!</code>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "For this tutorial, we are going to download the core Hadoop distribution and run Hadoop in _local standalone mode_:\n",
        "\n",
        "> ❝ _By default, Hadoop is configured to run in a non-distributed mode, as a single Java process._ ❞\n",
        "\n",
        "(see [https://hadoop.apache.org/docs/stable/.../Standalone_Operation](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Standalone_Operation))\n",
        "\n",
        "We are going to run a MapReduce job using MapReduce's [streaming application](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Hadoop_Streaming). This is not to be confused with real-time streaming:\n",
        "\n",
        "> ❝ _Hadoop streaming is a utility that comes with the Hadoop distribution. The utility allows you to create and run Map/Reduce jobs with any executable or script as the mapper and/or the reducer._ ❞\n",
        "\n",
        "MapReduce streaming defaults to using [`IdentityMapper`](https://hadoop.apache.org/docs/stable/api/index.html) and [`IdentityReducer`](https://hadoop.apache.org/docs/stable/api/index.html), thus eliminating the need for explicit specification of a mapper or reducer. Finally, we show how to run a map-only job by setting `mapreduce.job.reduce` equal to $0$.\n",
        "\n",
        "Both input and output are standard files since Hadoop's default filesystem is the regular file system, as specified by the `fs.defaultFS` property in [core-default.xml](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/core-default.xml)).\n"
      ],
      "metadata": {
        "id": "GzbmlR27wh6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download core Hadoop"
      ],
      "metadata": {
        "id": "uUbM5R0GwwYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HADOOP_URL = \"https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.4.0.tar.gz\"\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import tarfile\n",
        "\n",
        "def download_and_extract_targz(url):\n",
        "    response = requests.get(url)\n",
        "    filename = url.rsplit('/', 1)[-1]\n",
        "    HADOOP_HOME = filename[:-7]\n",
        "    # set HADOOP_HOME environment variable\n",
        "    os.environ['HADOOP_HOME'] = HADOOP_HOME\n",
        "    if os.path.isdir(HADOOP_HOME):\n",
        "      print(\"Not downloading, Hadoop folder {} already exists\".format(HADOOP_HOME))\n",
        "      return\n",
        "    if response.status_code == 200:\n",
        "        with open(filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        with tarfile.open(filename, 'r:gz') as tar_ref:\n",
        "            extract_path = tar_ref.extractall(path='.')\n",
        "            # Get the names of all members (files and directories) in the archive\n",
        "            all_members = tar_ref.getnames()\n",
        "            # If there is a top-level directory, get its name\n",
        "            if all_members:\n",
        "              top_level_directory = all_members[0]\n",
        "              print(f\"ZIP file downloaded and extracted successfully. Contents saved at: {top_level_directory}\")\n",
        "    else:\n",
        "        print(f\"Failed to download ZIP file. Status code: {response.status_code}\")\n",
        "\n",
        "\n",
        "download_and_extract_targz(HADOOP_URL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDgQtQlzw8bL",
        "outputId": "b8259622-3ba3-4074-8480-37dab685f88f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ZIP file downloaded and extracted successfully. Contents saved at: hadoop-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set environment variables"
      ],
      "metadata": {
        "id": "3yvb5cw9xEbh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set `HADOOP_HOME` and `PATH`"
      ],
      "metadata": {
        "id": "u6lkrz1dxIiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HADOOP_HOME was set earlier when downloading Hadoop distribution\n",
        "print(\"HADOOP_HOME is {}\".format(os.environ['HADOOP_HOME']))\n",
        "\n",
        "os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])\n",
        "print(\"PATH is {}\".format(os.environ['PATH']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7maAwaFxBT_",
        "outputId": "1ff277e2-6f41-4f88-f777-e33bd5998aa4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HADOOP_HOME is hadoop-3.4.0\n",
            "PATH is hadoop-3.4.0/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set `JAVA_HOME`\n",
        "\n",
        "While Java is readily available on Google Colab, we consider the broader scenario of an Ubuntu machine. In this case, we ensure compatibility by installing Java, specifically opting for the `openjdk-19-jre-headless` version."
      ],
      "metadata": {
        "id": "4kzJ8cNoxPyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# set variable JAVA_HOME (install Java if necessary)\n",
        "def is_java_installed():\n",
        "    os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "    return os.environ['JAVA_HOME']\n",
        "\n",
        "def install_java():\n",
        "    # Uncomment and modify the desired version\n",
        "    # java_version= 'openjdk-11-jre-headless'\n",
        "    # java_version= 'default-jre'\n",
        "    # java_version= 'openjdk-17-jre-headless'\n",
        "    # java_version= 'openjdk-18-jre-headless'\n",
        "    java_version= 'openjdk-19-jre-headless'\n",
        "\n",
        "    print(f\"Java not found. Installing {java_version} ... (this might take a while)\")\n",
        "    try:\n",
        "        cmd = f\"apt install -y {java_version}\"\n",
        "        subprocess_output = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "        stdout_result = subprocess_output.stdout\n",
        "        # Process the results as needed\n",
        "        print(\"Done installing Java {}\".format(java_version))\n",
        "        os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "        print(\"JAVA_HOME is {}\".format(os.environ['JAVA_HOME']))\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # Handle the error if the command returns a non-zero exit code\n",
        "        print(\"Command failed with return code {}\".format(e.returncode))\n",
        "        print(\"stdout: {}\".format(e.stdout))\n",
        "\n",
        "# Install Java if not available\n",
        "if is_java_installed():\n",
        "    print(\"Java is already installed: {}\".format(os.environ['JAVA_HOME']))\n",
        "else:\n",
        "    print(\"Installing Java\")\n",
        "    install_java()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SauFHVPOxL-Y",
        "outputId": "778e5fe5-b72f-4c9b-a2b7-6df57a54a1c9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Java is already installed: /usr/lib/jvm/java-11-openjdk-amd64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run a MapReduce job with Hadoop streaming"
      ],
      "metadata": {
        "id": "6HFPVX84xbNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a file\n",
        "\n",
        "Write the string\"Hello, World!\" to a local file.<p>**Note:** you will be writing to the file `./hello.txt` in your current directory (denoted by `./`)."
      ],
      "metadata": {
        "id": "_yVa55X1xmOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"Hello, World!\">./hello.txt"
      ],
      "metadata": {
        "id": "9Jz7mJkcxYxw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Launch the MapReduce \"Hello, World!\" application\n",
        "\n",
        "Since the default filesystem is the local filesystem (as opposed to HDFS) we do not need to upload the local file `hello.txt` to HDFS.\n",
        "\n",
        "Run a MapReduce job with `/bin/cat` as a mapper and no reducer.\n",
        "\n",
        "**Note:** the first step of removing the output directory is necessary because MapReduce does not overwrite data folders by design."
      ],
      "metadata": {
        "id": "zSh_Kr5Bxvst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output \\\n",
        "    -mapper '/bin/cat'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nb5JryK9xpPA",
        "outputId": "6784e710-2814-4eb7-cbb5-df4de7ec3bf2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "rm: `my_output': No such file or directory\n",
            "2024-04-23 04:05:37,426 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-23 04:05:37,729 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-23 04:05:37,730 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-23 04:05:37,760 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-23 04:05:38,109 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-23 04:05:38,138 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-23 04:05:38,578 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local207118179_0001\n",
            "2024-04-23 04:05:38,578 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-23 04:05:38,859 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-23 04:05:38,861 INFO mapreduce.Job: Running job: job_local207118179_0001\n",
            "2024-04-23 04:05:38,888 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-23 04:05:38,891 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-23 04:05:38,904 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 04:05:38,905 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 04:05:38,960 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-23 04:05:38,969 INFO mapred.LocalJobRunner: Starting task: attempt_local207118179_0001_m_000000_0\n",
            "2024-04-23 04:05:39,003 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 04:05:39,006 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 04:05:39,031 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-23 04:05:39,042 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-04-23 04:05:39,059 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-04-23 04:05:39,155 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-04-23 04:05:39,155 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-04-23 04:05:39,155 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-04-23 04:05:39,155 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-04-23 04:05:39,155 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-04-23 04:05:39,160 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-04-23 04:05:39,166 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2024-04-23 04:05:39,173 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-04-23 04:05:39,175 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-04-23 04:05:39,175 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-04-23 04:05:39,176 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-04-23 04:05:39,176 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-04-23 04:05:39,177 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-04-23 04:05:39,179 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-04-23 04:05:39,179 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-04-23 04:05:39,179 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-04-23 04:05:39,180 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-04-23 04:05:39,181 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-04-23 04:05:39,181 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-04-23 04:05:39,206 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-23 04:05:39,212 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-23 04:05:39,212 INFO streaming.PipeMapRed: Records R/W=1/1\n",
            "2024-04-23 04:05:39,213 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-23 04:05:39,216 INFO mapred.LocalJobRunner: \n",
            "2024-04-23 04:05:39,217 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-04-23 04:05:39,217 INFO mapred.MapTask: Spilling map output\n",
            "2024-04-23 04:05:39,217 INFO mapred.MapTask: bufstart = 0; bufend = 15; bufvoid = 104857600\n",
            "2024-04-23 04:05:39,217 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
            "2024-04-23 04:05:39,230 INFO mapred.MapTask: Finished spill 0\n",
            "2024-04-23 04:05:39,260 INFO mapred.Task: Task:attempt_local207118179_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-23 04:05:39,264 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
            "2024-04-23 04:05:39,265 INFO mapred.Task: Task 'attempt_local207118179_0001_m_000000_0' done.\n",
            "2024-04-23 04:05:39,275 INFO mapred.Task: Final Counters for attempt_local207118179_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=854181\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=15\n",
            "\t\tMap output materialized bytes=23\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=432013312\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "2024-04-23 04:05:39,275 INFO mapred.LocalJobRunner: Finishing task: attempt_local207118179_0001_m_000000_0\n",
            "2024-04-23 04:05:39,276 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-23 04:05:39,280 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-04-23 04:05:39,280 INFO mapred.LocalJobRunner: Starting task: attempt_local207118179_0001_r_000000_0\n",
            "2024-04-23 04:05:39,291 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 04:05:39,291 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 04:05:39,292 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-23 04:05:39,298 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@103a55ec\n",
            "2024-04-23 04:05:39,303 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-23 04:05:39,329 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-04-23 04:05:39,338 INFO reduce.EventFetcher: attempt_local207118179_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-04-23 04:05:39,378 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local207118179_0001_m_000000_0 decomp: 19 len: 23 to MEMORY\n",
            "2024-04-23 04:05:39,382 INFO reduce.InMemoryMapOutput: Read 19 bytes from map-output for attempt_local207118179_0001_m_000000_0\n",
            "2024-04-23 04:05:39,387 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 19, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->19\n",
            "2024-04-23 04:05:39,391 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-04-23 04:05:39,392 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-23 04:05:39,392 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-04-23 04:05:39,421 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-23 04:05:39,422 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
            "2024-04-23 04:05:39,423 INFO reduce.MergeManagerImpl: Merged 1 segments, 19 bytes to disk to satisfy reduce memory limit\n",
            "2024-04-23 04:05:39,424 INFO reduce.MergeManagerImpl: Merging 1 files, 23 bytes from disk\n",
            "2024-04-23 04:05:39,425 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-04-23 04:05:39,425 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-23 04:05:39,426 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
            "2024-04-23 04:05:39,427 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-23 04:05:39,435 INFO mapred.Task: Task:attempt_local207118179_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-04-23 04:05:39,436 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-23 04:05:39,437 INFO mapred.Task: Task attempt_local207118179_0001_r_000000_0 is allowed to commit now\n",
            "2024-04-23 04:05:39,438 INFO output.FileOutputCommitter: Saved output of task 'attempt_local207118179_0001_r_000000_0' to file:/content/my_output\n",
            "2024-04-23 04:05:39,439 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-04-23 04:05:39,440 INFO mapred.Task: Task 'attempt_local207118179_0001_r_000000_0' done.\n",
            "2024-04-23 04:05:39,440 INFO mapred.Task: Final Counters for attempt_local207118179_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141992\n",
            "\t\tFILE: Number of bytes written=854231\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=23\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=1\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=432013312\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-04-23 04:05:39,440 INFO mapred.LocalJobRunner: Finishing task: attempt_local207118179_0001_r_000000_0\n",
            "2024-04-23 04:05:39,441 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-04-23 04:05:39,868 INFO mapreduce.Job: Job job_local207118179_0001 running in uber mode : false\n",
            "2024-04-23 04:05:39,870 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-04-23 04:05:39,871 INFO mapreduce.Job: Job job_local207118179_0001 completed successfully\n",
            "2024-04-23 04:05:39,884 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=283906\n",
            "\t\tFILE: Number of bytes written=1708412\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=15\n",
            "\t\tMap output materialized bytes=23\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=23\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=2\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=864026624\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-04-23 04:05:39,884 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result\n",
        "\n",
        "If the job executed successfully, an empty file named `_SUCCESS` is expected to be present in the output directory `my_output`.\n",
        "\n",
        "Verify the success of the MapReduce job by checking for the presence of the `_SUCCESS` file."
      ],
      "metadata": {
        "id": "OB_fX9u5x55y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "echo \"Check if MapReduce job was successful\"\n",
        "hdfs dfs -test -e my_output/_SUCCESS\n",
        "if [ $? -eq 0 ]; then\n",
        "\techo \"_SUCCESS exists!\"\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnvEvYDfx2g4",
        "outputId": "ba3ccaad-2e3a-4f07-ed06-5e138b0151cb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check if MapReduce job was successful\n",
            "_SUCCESS exists!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** `hdfs dfs -ls` is the same as `ls` since the default filesystem is the local filesystem."
      ],
      "metadata": {
        "id": "BLMnBh44x_YR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -ls my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufAfmGUvx8jW",
        "outputId": "306f8947-3dec-4f37-a959-de1d9262c7d2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2024-04-23 04:05 my_output/_SUCCESS\n",
            "-rw-r--r--   1 root root         15 2024-04-23 04:05 my_output/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnKSahPzyCAn",
        "outputId": "4d636ff8-2e6d-440a-9618-bc1f2695e7f2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "-rw-r--r-- 1 root root 15 Apr 23 04:05 part-00000\n",
            "-rw-r--r-- 1 root root  0 Apr 23 04:05 _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The actual output of the MapReduce job is contained in the file `part-00000` in the output directory."
      ],
      "metadata": {
        "id": "v9LmpcaMyG23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eL-Clat5yD8I",
        "outputId": "88ea79c5-f766-4591-8dbe-5eef8a7dd69e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, World!\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MapReduce without specifying mapper or reducer\n",
        "\n",
        "In the previous example, we have seen how to run a MapReduce job without specifying any reducer.\n",
        "\n",
        "Since the only required options for `mapred streaming` are `input` and `output`, we can also run a MapReduce job without specifying a mapper."
      ],
      "metadata": {
        "id": "AmpHr_HyyMnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mapred streaming -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPWL1AiXyJac",
        "outputId": "df096968-4bc0-4f9b-e97d-fd3d53bae021"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-23 04:05:46,837 ERROR streaming.StreamJob: Unrecognized option: -h\n",
            "Usage: $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar [options]\n",
            "Options:\n",
            "  -input          <path> DFS input file(s) for the Map step.\n",
            "  -output         <path> DFS output directory for the Reduce step.\n",
            "  -mapper         <cmd|JavaClassName> Optional. Command to be run as mapper.\n",
            "  -combiner       <cmd|JavaClassName> Optional. Command to be run as combiner.\n",
            "  -reducer        <cmd|JavaClassName> Optional. Command to be run as reducer.\n",
            "  -file           <file> Optional. File/dir to be shipped in the Job jar file.\n",
            "                  Deprecated. Use generic option \"-files\" instead.\n",
            "  -inputformat    <TextInputFormat(default)|SequenceFileAsTextInputFormat|JavaClassName>\n",
            "                  Optional. The input format class.\n",
            "  -outputformat   <TextOutputFormat(default)|JavaClassName>\n",
            "                  Optional. The output format class.\n",
            "  -partitioner    <JavaClassName>  Optional. The partitioner class.\n",
            "  -numReduceTasks <num> Optional. Number of reduce tasks.\n",
            "  -inputreader    <spec> Optional. Input recordreader spec.\n",
            "  -cmdenv         <n>=<v> Optional. Pass env.var to streaming commands.\n",
            "  -mapdebug       <cmd> Optional. To run this script when a map task fails.\n",
            "  -reducedebug    <cmd> Optional. To run this script when a reduce task fails.\n",
            "  -io             <identifier> Optional. Format to use for input to and output\n",
            "                  from mapper/reducer commands\n",
            "  -lazyOutput     Optional. Lazily create Output.\n",
            "  -background     Optional. Submit the job and don't wait till it completes.\n",
            "  -verbose        Optional. Print verbose output.\n",
            "  -info           Optional. Print detailed usage.\n",
            "  -help           Optional. Print help message.\n",
            "\n",
            "Generic options supported are:\n",
            "-conf <configuration file>        specify an application configuration file\n",
            "-D <property=value>               define a value for a given property\n",
            "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n",
            "-jt <local|resourcemanager:port>  specify a ResourceManager\n",
            "-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n",
            "-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n",
            "-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n",
            "\n",
            "The general command line syntax is:\n",
            "command [genericOptions] [commandOptions]\n",
            "\n",
            "\n",
            "For more details about these options:\n",
            "Use $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar -info\n",
            "\n",
            "Try -help for more information\n",
            "Streaming Command Failed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5H2MkIUPyQc2",
        "outputId": "ab84298e-5bff-4de4-e461-16eb4c9467a1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-23 04:05:48,767 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-04-23 04:05:51,013 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-23 04:05:51,187 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-23 04:05:51,187 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-23 04:05:51,217 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-23 04:05:51,499 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-23 04:05:51,522 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-23 04:05:51,826 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local688102081_0001\n",
            "2024-04-23 04:05:51,826 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-23 04:05:52,098 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-23 04:05:52,100 INFO mapreduce.Job: Running job: job_local688102081_0001\n",
            "2024-04-23 04:05:52,112 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-23 04:05:52,115 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-23 04:05:52,126 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 04:05:52,126 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 04:05:52,225 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-23 04:05:52,237 INFO mapred.LocalJobRunner: Starting task: attempt_local688102081_0001_m_000000_0\n",
            "2024-04-23 04:05:52,280 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 04:05:52,281 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 04:05:52,310 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-23 04:05:52,321 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-04-23 04:05:52,338 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-04-23 04:05:52,431 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-04-23 04:05:52,432 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-04-23 04:05:52,432 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-04-23 04:05:52,432 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-04-23 04:05:52,432 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-04-23 04:05:52,437 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-04-23 04:05:52,446 INFO mapred.LocalJobRunner: \n",
            "2024-04-23 04:05:52,447 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-04-23 04:05:52,447 INFO mapred.MapTask: Spilling map output\n",
            "2024-04-23 04:05:52,447 INFO mapred.MapTask: bufstart = 0; bufend = 22; bufvoid = 104857600\n",
            "2024-04-23 04:05:52,447 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
            "2024-04-23 04:05:52,456 INFO mapred.MapTask: Finished spill 0\n",
            "2024-04-23 04:05:52,475 INFO mapred.Task: Task:attempt_local688102081_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-23 04:05:52,480 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+14\n",
            "2024-04-23 04:05:52,480 INFO mapred.Task: Task 'attempt_local688102081_0001_m_000000_0' done.\n",
            "2024-04-23 04:05:52,491 INFO mapred.Task: Final Counters for attempt_local688102081_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=852085\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=22\n",
            "\t\tMap output materialized bytes=30\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=353370112\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "2024-04-23 04:05:52,491 INFO mapred.LocalJobRunner: Finishing task: attempt_local688102081_0001_m_000000_0\n",
            "2024-04-23 04:05:52,492 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-23 04:05:52,496 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-04-23 04:05:52,500 INFO mapred.LocalJobRunner: Starting task: attempt_local688102081_0001_r_000000_0\n",
            "2024-04-23 04:05:52,511 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 04:05:52,511 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 04:05:52,511 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-23 04:05:52,518 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@67c923c3\n",
            "2024-04-23 04:05:52,521 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-23 04:05:52,553 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-04-23 04:05:52,566 INFO reduce.EventFetcher: attempt_local688102081_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-04-23 04:05:52,609 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local688102081_0001_m_000000_0 decomp: 26 len: 30 to MEMORY\n",
            "2024-04-23 04:05:52,614 INFO reduce.InMemoryMapOutput: Read 26 bytes from map-output for attempt_local688102081_0001_m_000000_0\n",
            "2024-04-23 04:05:52,619 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 26, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->26\n",
            "2024-04-23 04:05:52,623 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-04-23 04:05:52,624 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-23 04:05:52,624 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-04-23 04:05:52,632 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-23 04:05:52,632 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n",
            "2024-04-23 04:05:52,634 INFO reduce.MergeManagerImpl: Merged 1 segments, 26 bytes to disk to satisfy reduce memory limit\n",
            "2024-04-23 04:05:52,635 INFO reduce.MergeManagerImpl: Merging 1 files, 30 bytes from disk\n",
            "2024-04-23 04:05:52,636 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-04-23 04:05:52,636 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-23 04:05:52,637 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n",
            "2024-04-23 04:05:52,637 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-23 04:05:52,650 INFO mapred.Task: Task:attempt_local688102081_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-04-23 04:05:52,651 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-23 04:05:52,652 INFO mapred.Task: Task attempt_local688102081_0001_r_000000_0 is allowed to commit now\n",
            "2024-04-23 04:05:52,653 INFO output.FileOutputCommitter: Saved output of task 'attempt_local688102081_0001_r_000000_0' to file:/content/my_output\n",
            "2024-04-23 04:05:52,656 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-04-23 04:05:52,656 INFO mapred.Task: Task 'attempt_local688102081_0001_r_000000_0' done.\n",
            "2024-04-23 04:05:52,658 INFO mapred.Task: Final Counters for attempt_local688102081_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=142006\n",
            "\t\tFILE: Number of bytes written=852143\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=30\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=1\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=353370112\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-04-23 04:05:52,658 INFO mapred.LocalJobRunner: Finishing task: attempt_local688102081_0001_r_000000_0\n",
            "2024-04-23 04:05:52,658 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-04-23 04:05:53,107 INFO mapreduce.Job: Job job_local688102081_0001 running in uber mode : false\n",
            "2024-04-23 04:05:53,108 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-04-23 04:05:53,110 INFO mapreduce.Job: Job job_local688102081_0001 completed successfully\n",
            "2024-04-23 04:05:53,121 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=283920\n",
            "\t\tFILE: Number of bytes written=1704228\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=22\n",
            "\t\tMap output materialized bytes=30\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=30\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=2\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=706740224\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-04-23 04:05:53,121 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result"
      ],
      "metadata": {
        "id": "v7Ks3e96yXuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "echo \"Check if MapReduce job was successful\"\n",
        "hdfs dfs -test -e my_output/_SUCCESS\n",
        "if [ $? -eq 0 ]; then\n",
        "\techo \"_SUCCESS exists!\"\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWAXvG0_yThc",
        "outputId": "a6967cb5-e668-454f-d117-789a1fb30405"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check if MapReduce job was successful\n",
            "_SUCCESS exists!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show output"
      ],
      "metadata": {
        "id": "t40GgJ2Hya9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5APWEgoyaRS",
        "outputId": "b64b0834-7053-4484-a344-2e32e8f45737"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tHello, World!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happened here is that not having defined any mapper or reducer, the \"Identity\" mapper ([IdentityMapper](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityMapper.html)) and reducer ([IdentityReducer](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityReducer.html)) were used by default (see [Streaming command options](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Streaming_Command_Options))."
      ],
      "metadata": {
        "id": "mzfaMVKqyjpC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run a map-only MapReduce job\n",
        "\n",
        "Not specifying mapper and reducer in the MapReduce job submission does not mean that MapReduce isn't going to run the mapper and reducer steps, it is simply going to use the Identity mapper and reducer.\n",
        "\n",
        "To run a MapReduce job _without_ reducer one needs to use the generic option\n",
        "\n",
        "    \\-D mapreduce.job.reduces=0\n",
        "\n",
        "(see [specifying map-only jobs](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Specifying_Map-Only_Jobs))."
      ],
      "metadata": {
        "id": "lzIuWv7Myndc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -D mapreduce.job.reduces=0 \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdwKWyVRye27",
        "outputId": "c10cc905-352a-4731-da7a-c823a550b7fd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-23 04:05:58,333 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-04-23 04:06:00,643 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-23 04:06:00,821 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-23 04:06:00,822 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-23 04:06:00,854 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-23 04:06:01,153 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-23 04:06:01,186 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-23 04:06:01,505 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local628769955_0001\n",
            "2024-04-23 04:06:01,505 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-23 04:06:01,768 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-23 04:06:01,770 INFO mapreduce.Job: Running job: job_local628769955_0001\n",
            "2024-04-23 04:06:01,781 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-23 04:06:01,783 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-23 04:06:01,792 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 04:06:01,792 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 04:06:01,848 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-23 04:06:01,861 INFO mapred.LocalJobRunner: Starting task: attempt_local628769955_0001_m_000000_0\n",
            "2024-04-23 04:06:01,916 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 04:06:01,921 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 04:06:01,957 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-23 04:06:01,969 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-04-23 04:06:01,992 INFO mapred.MapTask: numReduceTasks: 0\n",
            "2024-04-23 04:06:02,038 INFO mapred.LocalJobRunner: \n",
            "2024-04-23 04:06:02,050 INFO mapred.Task: Task:attempt_local628769955_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-23 04:06:02,052 INFO mapred.LocalJobRunner: \n",
            "2024-04-23 04:06:02,052 INFO mapred.Task: Task attempt_local628769955_0001_m_000000_0 is allowed to commit now\n",
            "2024-04-23 04:06:02,054 INFO output.FileOutputCommitter: Saved output of task 'attempt_local628769955_0001_m_000000_0' to file:/content/my_output\n",
            "2024-04-23 04:06:02,055 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+14\n",
            "2024-04-23 04:06:02,055 INFO mapred.Task: Task 'attempt_local628769955_0001_m_000000_0' done.\n",
            "2024-04-23 04:06:02,066 INFO mapred.Task: Final Counters for attempt_local628769955_0001_m_000000_0: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=852049\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=20\n",
            "\t\tTotal committed heap usage (bytes)=395313152\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-04-23 04:06:02,066 INFO mapred.LocalJobRunner: Finishing task: attempt_local628769955_0001_m_000000_0\n",
            "2024-04-23 04:06:02,067 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-23 04:06:02,777 INFO mapreduce.Job: Job job_local628769955_0001 running in uber mode : false\n",
            "2024-04-23 04:06:02,780 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2024-04-23 04:06:02,783 INFO mapreduce.Job: Job job_local628769955_0001 completed successfully\n",
            "2024-04-23 04:06:02,790 INFO mapreduce.Job: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=852049\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=20\n",
            "\t\tTotal committed heap usage (bytes)=395313152\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-04-23 04:06:02,790 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result"
      ],
      "metadata": {
        "id": "QZIE9yXOyyHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Dt3tUI0yu5e",
        "outputId": "83e4ee5c-9541-4972-d94b-7f2334f66dc2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tHello, World!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why a map-only application?\n",
        "\n",
        "The advantage of a map-only job is that the sorting and shuffling phases are skipped, so if you do not need that remember to specify `-D mapreduce.job.reduces=0 `.\n",
        "\n",
        "On the other hand, a MapReduce job even with the default `IdentityReducer` will deliver sorted results because the data passed from the mapper to the reducer always gets sorted.\n"
      ],
      "metadata": {
        "id": "hUGEUv99y3cM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improved version of the MapReduce \"Hello, World!\" application\n",
        "\n",
        "Taking into account the previous considerations, here's a more efficient version of the 'Hello, World!' application that bypasses the shuffling and sorting step."
      ],
      "metadata": {
        "id": "FhVVFEdKzGcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -D mapreduce.job.reduces=0 \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output \\\n",
        "    -mapper '/bin/cat'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLgMXX2jy0vC",
        "outputId": "aee1efd0-e28d-48ad-dd26-8d7581c80f18"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-23 04:06:06,670 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-04-23 04:06:09,356 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-23 04:06:09,708 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-23 04:06:09,708 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-23 04:06:09,752 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-23 04:06:10,282 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-23 04:06:10,341 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-23 04:06:10,847 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local521887015_0001\n",
            "2024-04-23 04:06:10,851 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-23 04:06:11,318 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-23 04:06:11,320 INFO mapreduce.Job: Running job: job_local521887015_0001\n",
            "2024-04-23 04:06:11,330 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-23 04:06:11,336 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-23 04:06:11,359 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 04:06:11,366 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 04:06:11,491 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-23 04:06:11,497 INFO mapred.LocalJobRunner: Starting task: attempt_local521887015_0001_m_000000_0\n",
            "2024-04-23 04:06:11,547 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 04:06:11,550 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 04:06:11,602 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-23 04:06:11,633 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-04-23 04:06:11,662 INFO mapred.MapTask: numReduceTasks: 0\n",
            "2024-04-23 04:06:11,688 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2024-04-23 04:06:11,697 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-04-23 04:06:11,699 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-04-23 04:06:11,708 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-04-23 04:06:11,709 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-04-23 04:06:11,710 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-04-23 04:06:11,710 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-04-23 04:06:11,713 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-04-23 04:06:11,720 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-04-23 04:06:11,720 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-04-23 04:06:11,721 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-04-23 04:06:11,722 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-04-23 04:06:11,723 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-04-23 04:06:11,757 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-23 04:06:11,759 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-23 04:06:11,759 INFO streaming.PipeMapRed: Records R/W=1/1\n",
            "2024-04-23 04:06:11,760 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-23 04:06:11,762 INFO mapred.LocalJobRunner: \n",
            "2024-04-23 04:06:11,775 INFO mapred.Task: Task:attempt_local521887015_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-23 04:06:11,778 INFO mapred.LocalJobRunner: \n",
            "2024-04-23 04:06:11,778 INFO mapred.Task: Task attempt_local521887015_0001_m_000000_0 is allowed to commit now\n",
            "2024-04-23 04:06:11,790 INFO output.FileOutputCommitter: Saved output of task 'attempt_local521887015_0001_m_000000_0' to file:/content/my_output\n",
            "2024-04-23 04:06:11,792 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
            "2024-04-23 04:06:11,792 INFO mapred.Task: Task 'attempt_local521887015_0001_m_000000_0' done.\n",
            "2024-04-23 04:06:11,803 INFO mapred.Task: Final Counters for attempt_local521887015_0001_m_000000_0: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=855001\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=403701760\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-04-23 04:06:11,803 INFO mapred.LocalJobRunner: Finishing task: attempt_local521887015_0001_m_000000_0\n",
            "2024-04-23 04:06:11,804 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-23 04:06:12,331 INFO mapreduce.Job: Job job_local521887015_0001 running in uber mode : false\n",
            "2024-04-23 04:06:12,333 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2024-04-23 04:06:12,336 INFO mapreduce.Job: Job job_local521887015_0001 completed successfully\n",
            "2024-04-23 04:06:12,346 INFO mapreduce.Job: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=855001\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=403701760\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-04-23 04:06:12,347 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sa1UDPr6zKKw",
        "outputId": "c4434908-1784-4935-85a0-d843d8ba764f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, World!\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Nguyen Quy Dang - 22022500\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLubhjbZOxQ-",
        "outputId": "b62810f9-4b08-47ee-dda2-ae2c84e663d2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nguyen Quy Dang - 22022500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HzT_diGnOz_g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}